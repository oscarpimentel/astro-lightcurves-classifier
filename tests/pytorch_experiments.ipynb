{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "#import torch.nn\n",
    "\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        print('grad_output',grad_output.shape)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "            #grad_input = grad_output.mm(weight.t())\n",
    "            print('grad_input',grad_input.shape)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            print('grad_weight',grad_weight.shape)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "    \n",
    "output_features = 4\n",
    "input_features = 8\n",
    "x = (torch.ones((20,input_features))+1)#.unsqueeze(0)\n",
    "w = (torch.ones((output_features,input_features))+2)#.unsqueeze(0)\n",
    "b = (torch.ones((1))*13).squeeze()\n",
    "print(b.shape)\n",
    "x.requires_grad_(True)\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "print(x.shape)\n",
    "print(w.shape)\n",
    "y = LinearFunction.apply(x,w)#,b)\n",
    "#y = y.mm(x)\n",
    "print('y',y.shape)\n",
    "t = torch.ones_like(y)#(y.size())\n",
    "print(t.shape)\n",
    "y.backward(t)\n",
    "\n",
    "print('grad x',x.grad)\n",
    "print('grad w',w.grad)\n",
    "print('grad b',b.grad)\n",
    "print('-'*50)\n",
    "print('grad x',1*w)\n",
    "print('grad w',1*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "#import torch.nn\n",
    "\n",
    "class Att_op(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, v, q, weigths): # bxn x bxn nxn\n",
    "        ctx.save_for_backward(v,q,weigths)\n",
    "        #print('weigths',weigths.shape)\n",
    "        #print('v',v.shape)\n",
    "        vv = torch.mm(v, weigths)\n",
    "        #print('vv',vv.shape)\n",
    "        out = torch.bmm(vv.unsqueeze(1),q.unsqueeze(-1)).squeeze() # bx1xn x bxnx1\n",
    "        return out\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output, grad_output\n",
    "    \n",
    "class Att_F(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, values, query, weigths): # bxtxn x bxn n\n",
    "        ctx.save_for_backward(values, query)\n",
    "        b = values.shape[0]\n",
    "        time_steps = values.shape[1]\n",
    "        output = torch.zeros((b,time_steps,1))\n",
    "        for i in range(time_steps):\n",
    "            v = values[:,i,:]\n",
    "            q = query\n",
    "            out = Att_op.apply(v,q,weigths)\n",
    "            #print('v',v.shape) # bxn\n",
    "            #print('query',query.shape) # bxn\n",
    "            #print('out',out.shape) # bx1\n",
    "            output[:,i,0] = out\n",
    "        output = F.softmax(output,dim=1)\n",
    "        return output # bxtx1\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        return grad_output, grad_output, grad_output\n",
    "        \n",
    "\n",
    "b = 3\n",
    "input_features = 2\n",
    "values = (torch.ones((b,3,input_features))*1)#.unsqueeze(0)\n",
    "weigths = nn.Parameter(torch.ones((input_features,input_features)))\n",
    "#weigths = (torch.ones((input_features,input_features))*1)#.unsqueeze(0)\n",
    "print('weigths',weigths)\n",
    "\n",
    "values[0,0,0] = -1\n",
    "values[1,0,0] = -5\n",
    "print(values.shape)\n",
    "print(values)\n",
    "query = (torch.ones((b,input_features))*0.5)#.unsqueeze(0)\n",
    "print(query.shape)\n",
    "print(query)\n",
    "att = Att_F.apply(values, query, weigths)\n",
    "print('att',att.shape)\n",
    "print(att[1])\n",
    "att = att.sum()\n",
    "grads = torch.ones(input_features)\n",
    "print('grads',grads.shape)\n",
    "#att.backward(grads)\n",
    "att.backward()\n",
    "print('-'*50)\n",
    "print(weigths.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionContext(nn.Module):\n",
    "    def __init__(self, input_features, output_features, head_size):\n",
    "        super(MultiHeadAttentionContext, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.concat_size = input_features*head_size\n",
    "        self.v_linear = nn.ModuleList([nn.Linear(input_features,input_features) for i in range(head_size)])\n",
    "        self.k_linear = nn.ModuleList([nn.Linear(input_features,input_features) for i in range(head_size)])\n",
    "        self.q_linear = nn.ModuleList([nn.Linear(input_features,input_features) for i in range(head_size)])\n",
    "        self.concat_linear = nn.Linear(self.concat_size,output_features)\n",
    "\n",
    "    def forward(self, values, query): # txn n\n",
    "        b = values.shape[0]\n",
    "        #tt = values.shape[1]\n",
    "        multihead = torch.zeros(b,self.input_features,self.head_size)#.unsqueeze(-1) # txnxm\n",
    "        #print('multihead',multihead.shape)\n",
    "        for i in range(head_size):\n",
    "            vl = self.v_linear[i](values)\n",
    "            kl = self.k_linear[i](values)\n",
    "            ql = self.q_linear[i](query)\n",
    "            #print(i,'-'*20);print('vl',vl.shape);print('ql',ql.shape)\n",
    "            print(i,'-'*20);print('vl',vl.max());print('ql',ql.max())\n",
    "            qk = torch.bmm(kl,ql.unsqueeze(-1)) # n * txn = t\n",
    "            #print('qk',qk.shape)\n",
    "            soft = F.softmax(qk,dim=1).expand_as(vl)\n",
    "            #print('soft',soft.shape)\n",
    "            #print('soft',soft[2,:])\n",
    "            #print(vl[0,:,:2]) # b,t,n\n",
    "            #print(soft[0,:]) # b,t\n",
    "            att = (vl*soft).sum(dim=1)\n",
    "            #att = vl[:,:,:]*soft[:,:,None]\n",
    "            #print('att',att.shape)\n",
    "            #print('att',att[0,:,:2])\n",
    "            multihead[:,:,i] = att\n",
    "        #print('multihead',multihead[0])\n",
    "        multihead = multihead.view(b,-1)\n",
    "        #print('multihead',multihead)\n",
    "        out = self.concat_linear(multihead)\n",
    "        #print('out',out.shape)\n",
    "        return out\n",
    "    \n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, input_features, output_features, head_size):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.head_size = head_size\n",
    "        self.attentionContext = MultiHeadAttentionContext(input_features, output_features, head_size)\n",
    "        \n",
    "    def forward(self, values): # txn n\n",
    "        b = values.shape[0]\n",
    "        time_length = values.shape[1]\n",
    "        out = torch.zeros((b,time_length,self.output_features))\n",
    "        for i in range(time_length):\n",
    "            actual_query = values[:,i,:]\n",
    "            #print('actual_query',actual_query.shape)\n",
    "            #context = self.att(values,actual_query)\n",
    "            values_ = values[:,:i+1,:]\n",
    "            #print('values_',values_.shape)\n",
    "            context = self.attentionContext(values_,actual_query)\n",
    "            #print('context',context.shape)\n",
    "            out[:,i,:] = context\n",
    "            \n",
    "        return out\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "head_size=30\n",
    "input_features = 4\n",
    "output_features = 10\n",
    "attention1 = AttentionRNN(input_features, output_features, head_size)\n",
    "attention2 = AttentionRNN(output_features, output_features, head_size)\n",
    "final_linear = nn.Linear(30,1)\n",
    "#attention = AttentionRNN(input_features, output_features, head_size)\n",
    "print('attention',count_parameters(attention))\n",
    "b = 2\n",
    "t = 3\n",
    "x = torch.rand((b,t,input_features))\n",
    "#q = torch.rand((b,input_features))\n",
    "print(x.shape)\n",
    "#print(q.shape)\n",
    "x = attention1(x)\n",
    "x = attention2(x)\n",
    "print(x[0])\n",
    "x = x.view(b,-1)\n",
    "print(x.shape)\n",
    "x = final_linear(x)\n",
    "print(x.shape)\n",
    "x = x[0]\n",
    "x.backward()\n",
    "grad = attention2.attentionContext.concat_linear.weight.grad\n",
    "print(grad.shape)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 4\n",
    "output_features = 10\n",
    "l = nn.Linear(input_features, output_features)\n",
    "l2 = nn.Linear(output_features, 1)\n",
    "b = 2\n",
    "x = torch.rand((b,input_features))\n",
    "print(x[0])\n",
    "print(x[1])\n",
    "#q = torch.rand((b,input_features))\n",
    "print(x.shape)\n",
    "#print(q.shape)\n",
    "x = l(x)\n",
    "x = l2(x)\n",
    "print(x.shape)\n",
    "x = x[0]\n",
    "x.backward()\n",
    "grad = l.weight.grad\n",
    "print(grad.shape)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None\n",
    "\n",
    "t = torch.ones((2,2))\n",
    "t[0,0] = 2\n",
    "t.requires_grad_(True)\n",
    "y = MulConstant.apply(t,8)\n",
    "y2 = (y**2).sum()\n",
    "print(y2)\n",
    "y2.backward()\n",
    "print(t.grad)\n",
    "print(1*(y[0]*2)*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(8)\n",
    "t2 = torch.zeros((5,8))\n",
    "print(t.shape)\n",
    "t = torch.unsqueeze(t,0)\n",
    "print(t.shape)\n",
    "t = t.expand_as(t2)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([3.0, 2.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 7.0])\n",
    "ab_sum = a + b\n",
    "print(ab_sum)\n",
    "ab_res = (ab_sum*8).sum()\n",
    "ab_res.backward()\n",
    "print(ab_res)\n",
    "print('grad',a.grad)\n",
    "print('grad',ab_res.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn((2,2), requires_grad=True)\n",
    "a = torch.randn((1), requires_grad=True)\n",
    "print('a',a)\n",
    "a2 = a**2\n",
    "aa = torch.log(a2)\n",
    "#aa = a2/3\n",
    "b = aa.sum()\n",
    "print('b:',b)\n",
    "b.backward()\n",
    "print('grad:',a.grad)\n",
    "\n",
    "res = 1*(1/a2[0]w)*a[0]*2\n",
    "#res = 1*1/3*a[0]*2\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((1,8), requires_grad=True)#.unsqueeze(0)\n",
    "w = torch.randn((1,8), requires_grad=True)#.unsqueeze(0)\n",
    "print('x',x.shape)\n",
    "print('w',w.shape)\n",
    "y = x.mm(w.t())\n",
    "y2 = y**2\n",
    "y3 = 10-y2\n",
    "print('y',y.shape)\n",
    "#y = y.sum()\n",
    "print('y:',y)\n",
    "y3.backward()\n",
    "print('grad x:',x.grad)\n",
    "print('grad w:',w.grad)\n",
    "print('-'*50)\n",
    "print('grad x:',1*y*2*w)\n",
    "print('grad w:',1*y*2*x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
