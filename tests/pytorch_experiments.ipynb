{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([20, 8])\n",
      "torch.Size([4, 8])\n",
      "y torch.Size([20, 4])\n",
      "torch.Size([20, 4])\n",
      "grad_output torch.Size([20, 4])\n",
      "grad_input torch.Size([20, 8])\n",
      "grad_weight torch.Size([4, 8])\n",
      "grad x tensor([[12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.],\n",
      "        [12., 12., 12., 12., 12., 12., 12., 12.]])\n",
      "grad w tensor([[40., 40., 40., 40., 40., 40., 40., 40.],\n",
      "        [40., 40., 40., 40., 40., 40., 40., 40.],\n",
      "        [40., 40., 40., 40., 40., 40., 40., 40.],\n",
      "        [40., 40., 40., 40., 40., 40., 40., 40.]])\n",
      "grad b None\n",
      "--------------------------------------------------\n",
      "grad x tensor([[3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3., 3., 3., 3., 3.]], grad_fn=<MulBackward0>)\n",
      "grad w tensor([[2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2., 2., 2., 2.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "#import torch.nn\n",
    "\n",
    "# Inherit from Function\n",
    "class LinearFunction(Function):\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "        output = input.mm(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        print('grad_output',grad_output.shape)\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "            #grad_input = grad_output.mm(weight.t())\n",
    "            print('grad_input',grad_input.shape)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "            print('grad_weight',grad_weight.shape)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            \n",
    "            grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias\n",
    "    \n",
    "output_features = 4\n",
    "input_features = 8\n",
    "x = (torch.ones((20,input_features))+1)#.unsqueeze(0)\n",
    "w = (torch.ones((output_features,input_features))+2)#.unsqueeze(0)\n",
    "b = (torch.ones((1))*13).squeeze()\n",
    "print(b.shape)\n",
    "x.requires_grad_(True)\n",
    "w.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "print(x.shape)\n",
    "print(w.shape)\n",
    "y = LinearFunction.apply(x,w)#,b)\n",
    "#y = y.mm(x)\n",
    "print('y',y.shape)\n",
    "t = torch.ones_like(y)#(y.size())\n",
    "print(t.shape)\n",
    "y.backward(t)\n",
    "\n",
    "print('grad x',x.grad)\n",
    "print('grad w',w.grad)\n",
    "print('grad b',b.grad)\n",
    "print('-'*50)\n",
    "print('grad x',1*w)\n",
    "print('grad w',1*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weigths Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "torch.Size([3, 3, 2])\n",
      "tensor([[[-1.,  1.],\n",
      "         [ 1.,  1.],\n",
      "         [ 1.,  1.]],\n",
      "\n",
      "        [[-5.,  1.],\n",
      "         [ 1.,  1.],\n",
      "         [ 1.,  1.]],\n",
      "\n",
      "        [[ 1.,  1.],\n",
      "         [ 1.,  1.],\n",
      "         [ 1.,  1.]]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "att torch.Size([3, 3, 1])\n",
      "tensor([[0.0012],\n",
      "        [0.4994],\n",
      "        [0.4994]], grad_fn=<SelectBackward>)\n",
      "grads torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function Att_FBackward returned an invalid gradient at index 2 - got [3, 3, 1] but expected shape compatible with [2, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-951023b24126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grads'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#att.backward(grads)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweigths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-cpu/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-cpu/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Function Att_FBackward returned an invalid gradient at index 2 - got [3, 3, 1] but expected shape compatible with [2, 2]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "#import torch.nn\n",
    "\n",
    "class Att_op(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, v, q, weigths): # bxn x bxn nxn\n",
    "        ctx.save_for_backward(v,q,weigths)\n",
    "        #print('weigths',weigths.shape)\n",
    "        #print('v',v.shape)\n",
    "        vv = torch.mm(v, weigths)\n",
    "        #print('vv',vv.shape)\n",
    "        out = torch.bmm(vv.unsqueeze(1),q.unsqueeze(-1)).squeeze() # bx1xn x bxnx1\n",
    "        return out\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output, grad_output\n",
    "    \n",
    "class Att_F(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, values, query, weigths): # bxtxn x bxn n\n",
    "        ctx.save_for_backward(values, query)\n",
    "        b = values.shape[0]\n",
    "        time_steps = values.shape[1]\n",
    "        output = torch.zeros((b,time_steps,1))\n",
    "        for i in range(time_steps):\n",
    "            v = values[:,i,:]\n",
    "            q = query\n",
    "            out = Att_op.apply(v,q,weigths)\n",
    "            #print('v',v.shape) # bxn\n",
    "            #print('query',query.shape) # bxn\n",
    "            #print('out',out.shape) # bx1\n",
    "            output[:,i,0] = out\n",
    "        output = F.softmax(output,dim=1)\n",
    "        return output # bxtx1\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        return grad_output, grad_output, grad_output\n",
    "        \n",
    "\n",
    "b = 3\n",
    "input_features = 2\n",
    "values = (torch.ones((b,3,input_features))*1)#.unsqueeze(0)\n",
    "weigths = nn.Parameter(torch.ones((input_features,input_features)))\n",
    "#weigths = (torch.ones((input_features,input_features))*1)#.unsqueeze(0)\n",
    "print('weigths',weigths)\n",
    "\n",
    "values[0,0,0] = -1\n",
    "values[1,0,0] = -5\n",
    "print(values.shape)\n",
    "print(values)\n",
    "query = (torch.ones((b,input_features))*0.5)#.unsqueeze(0)\n",
    "print(query.shape)\n",
    "print(query)\n",
    "att = Att_F.apply(values, query, weigths)\n",
    "print('att',att.shape)\n",
    "print(att[1])\n",
    "att = att.sum()\n",
    "grads = torch.ones(input_features)\n",
    "print('grads',grads.shape)\n",
    "#att.backward(grads)\n",
    "att.backward()\n",
    "print('-'*50)\n",
    "print(weigths.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'attention' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bed1e6297bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mfinal_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m#attention = AttentionRNN(input_features, output_features, head_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attention'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'attention' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttentionContext(nn.Module):\n",
    "    def __init__(self, input_features, output_features, head_size):\n",
    "        super(MultiHeadAttentionContext, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.concat_size = input_features*head_size\n",
    "        self.v_linear = nn.ModuleList([nn.Linear(input_features,input_features) for i in range(head_size)])\n",
    "        self.k_linear = nn.ModuleList([nn.Linear(input_features,input_features) for i in range(head_size)])\n",
    "        self.q_linear = nn.ModuleList([nn.Linear(input_features,input_features) for i in range(head_size)])\n",
    "        self.concat_linear = nn.Linear(self.concat_size,output_features)\n",
    "\n",
    "    def forward(self, values, query): # txn n\n",
    "        b = values.shape[0]\n",
    "        #tt = values.shape[1]\n",
    "        multihead = torch.zeros(b,self.input_features,self.head_size)#.unsqueeze(-1) # txnxm\n",
    "        #print('multihead',multihead.shape)\n",
    "        for i in range(head_size):\n",
    "            vl = self.v_linear[i](values)\n",
    "            kl = self.k_linear[i](values)\n",
    "            ql = self.q_linear[i](query)\n",
    "            #print(i,'-'*20);print('vl',vl.shape);print('ql',ql.shape)\n",
    "            print(i,'-'*20);print('vl',vl.max());print('ql',ql.max())\n",
    "            qk = torch.bmm(kl,ql.unsqueeze(-1)) # n * txn = t\n",
    "            #print('qk',qk.shape)\n",
    "            soft = F.softmax(qk,dim=1).expand_as(vl)\n",
    "            #print('soft',soft.shape)\n",
    "            #print('soft',soft[2,:])\n",
    "            #print(vl[0,:,:2]) # b,t,n\n",
    "            #print(soft[0,:]) # b,t\n",
    "            att = (vl*soft).sum(dim=1)\n",
    "            #att = vl[:,:,:]*soft[:,:,None]\n",
    "            #print('att',att.shape)\n",
    "            #print('att',att[0,:,:2])\n",
    "            multihead[:,:,i] = att\n",
    "        #print('multihead',multihead[0])\n",
    "        multihead = multihead.view(b,-1)\n",
    "        #print('multihead',multihead)\n",
    "        out = self.concat_linear(multihead)\n",
    "        #print('out',out.shape)\n",
    "        return out\n",
    "    \n",
    "class AttentionRNN(nn.Module):\n",
    "    def __init__(self, input_features, output_features, head_size):\n",
    "        super(AttentionRNN, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        self.head_size = head_size\n",
    "        self.attentionContext = MultiHeadAttentionContext(input_features, output_features, head_size)\n",
    "        \n",
    "    def forward(self, values): # txn n\n",
    "        b = values.shape[0]\n",
    "        time_length = values.shape[1]\n",
    "        out = torch.zeros((b,time_length,self.output_features))\n",
    "        for i in range(time_length):\n",
    "            actual_query = values[:,i,:]\n",
    "            #print('actual_query',actual_query.shape)\n",
    "            #context = self.att(values,actual_query)\n",
    "            values_ = values[:,:i+1,:]\n",
    "            #print('values_',values_.shape)\n",
    "            context = self.attentionContext(values_,actual_query)\n",
    "            #print('context',context.shape)\n",
    "            out[:,i,:] = context\n",
    "            \n",
    "        return out\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "head_size=30\n",
    "input_features = 4\n",
    "output_features = 10\n",
    "attention1 = AttentionRNN(input_features, output_features, head_size)\n",
    "attention2 = AttentionRNN(output_features, output_features, head_size)\n",
    "final_linear = nn.Linear(30,1)\n",
    "#attention = AttentionRNN(input_features, output_features, head_size)\n",
    "print('attention',count_parameters(attention))\n",
    "b = 2\n",
    "t = 3\n",
    "x = torch.rand((b,t,input_features))\n",
    "#q = torch.rand((b,input_features))\n",
    "print(x.shape)\n",
    "#print(q.shape)\n",
    "x = attention1(x)\n",
    "x = attention2(x)\n",
    "print(x[0])\n",
    "x = x.view(b,-1)\n",
    "print(x.shape)\n",
    "x = final_linear(x)\n",
    "print(x.shape)\n",
    "x = x[0]\n",
    "x.backward()\n",
    "grad = attention2.attentionContext.concat_linear.weight.grad\n",
    "print(grad.shape)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9371, 0.7756, 0.9431, 0.5703])\n",
      "tensor([0.9754, 0.2133, 0.1869, 0.3648])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([10, 4])\n",
      "tensor([[-0.1231, -0.1019, -0.1239, -0.0749],\n",
      "        [ 0.2448,  0.2026,  0.2464,  0.1490],\n",
      "        [ 0.0971,  0.0804,  0.0977,  0.0591],\n",
      "        [ 0.0025,  0.0020,  0.0025,  0.0015],\n",
      "        [-0.0542, -0.0449, -0.0546, -0.0330],\n",
      "        [ 0.0961,  0.0795,  0.0967,  0.0585],\n",
      "        [ 0.0491,  0.0407,  0.0495,  0.0299],\n",
      "        [-0.1077, -0.0891, -0.1084, -0.0655],\n",
      "        [-0.2129, -0.1762, -0.2142, -0.1296],\n",
      "        [ 0.1786,  0.1478,  0.1797,  0.1087]])\n"
     ]
    }
   ],
   "source": [
    "input_features = 4\n",
    "output_features = 10\n",
    "l = nn.Linear(input_features, output_features)\n",
    "l2 = nn.Linear(output_features, 1)\n",
    "b = 2\n",
    "x = torch.rand((b,input_features))\n",
    "print(x[0])\n",
    "print(x[1])\n",
    "#q = torch.rand((b,input_features))\n",
    "print(x.shape)\n",
    "#print(q.shape)\n",
    "x = l(x)\n",
    "x = l2(x)\n",
    "print(x.shape)\n",
    "x = x[0]\n",
    "x.backward()\n",
    "grad = l.weight.grad\n",
    "print(grad.shape)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(448., grad_fn=<SumBackward0>)\n",
      "tensor([[256., 128.],\n",
      "        [128., 128.]])\n",
      "tensor([256., 128.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class MulConstant(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor, constant):\n",
    "        # ctx is a context object that can be used to stash information\n",
    "        # for backward computation\n",
    "        ctx.constant = constant\n",
    "        return tensor * constant\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # We return as many input gradients as there were arguments.\n",
    "        # Gradients of non-Tensor arguments to forward must be None.\n",
    "        return grad_output * ctx.constant, None\n",
    "\n",
    "t = torch.ones((2,2))\n",
    "t[0,0] = 2\n",
    "t.requires_grad_(True)\n",
    "y = MulConstant.apply(t,8)\n",
    "y2 = (y**2).sum()\n",
    "print(y2)\n",
    "y2.backward()\n",
    "print(t.grad)\n",
    "print(1*(y[0]*2)*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "torch.Size([1, 8])\n",
      "torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "t = torch.zeros(8)\n",
    "t2 = torch.zeros((5,8))\n",
    "print(t.shape)\n",
    "t = torch.unsqueeze(t,0)\n",
    "print(t.shape)\n",
    "t = t.expand_as(t2)\n",
    "print(t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7., 9.], grad_fn=<AddBackward0>)\n",
      "tensor(128., grad_fn=<SumBackward0>)\n",
      "grad tensor([8., 8.])\n",
      "grad None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([3.0, 2.0], requires_grad=True)\n",
    "b = torch.tensor([4.0, 7.0])\n",
    "ab_sum = a + b\n",
    "print(ab_sum)\n",
    "ab_res = (ab_sum*8).sum()\n",
    "ab_res.backward()\n",
    "print(ab_res)\n",
    "print('grad',a.grad)\n",
    "print('grad',ab_res.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a tensor([0.6115], requires_grad=True)\n",
      "b: tensor(-0.9837, grad_fn=<SumBackward0>)\n",
      "grad: tensor([3.2707])\n",
      "tensor(3.2707, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((2,2), requires_grad=True)\n",
    "a = torch.randn((1), requires_grad=True)\n",
    "print('a',a)\n",
    "a2 = a**2\n",
    "aa = torch.log(a2)\n",
    "#aa = a2/3\n",
    "b = aa.sum()\n",
    "print('b:',b)\n",
    "b.backward()\n",
    "print('grad:',a.grad)\n",
    "\n",
    "res = 1*(1/a2[0]w)*a[0]*2\n",
    "#res = 1*1/3*a[0]*2\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([1, 8])\n",
      "w torch.Size([1, 8])\n",
      "y torch.Size([1, 1])\n",
      "y: tensor([[0.6215]], grad_fn=<MmBackward>)\n",
      "grad x: tensor([[ 0.8652,  1.0976, -0.4235, -1.2149, -0.0071, -0.3101,  1.3825, -0.4072]])\n",
      "grad w: tensor([[-1.8467,  0.4197,  1.4980, -1.9762,  1.1061, -1.6743,  0.6661,  2.7042]])\n",
      "--------------------------------------------------\n",
      "grad x: tensor([[-0.8652, -1.0976,  0.4235,  1.2149,  0.0071,  0.3101, -1.3825,  0.4072]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "grad w: tensor([[ 1.8467, -0.4197, -1.4980,  1.9762, -1.1061,  1.6743, -0.6661, -2.7042]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((1,8), requires_grad=True)#.unsqueeze(0)\n",
    "w = torch.randn((1,8), requires_grad=True)#.unsqueeze(0)\n",
    "print('x',x.shape)\n",
    "print('w',w.shape)\n",
    "y = x.mm(w.t())\n",
    "y2 = y**2\n",
    "y3 = 10-y2\n",
    "print('y',y.shape)\n",
    "#y = y.sum()\n",
    "print('y:',y)\n",
    "y3.backward()\n",
    "print('grad x:',x.grad)\n",
    "print('grad w:',w.grad)\n",
    "print('-'*50)\n",
    "print('grad x:',1*y*2*w)\n",
    "print('grad w:',1*y*2*x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
