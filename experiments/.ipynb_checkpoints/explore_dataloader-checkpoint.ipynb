{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../') # or just install the module\n",
    "sys.path.append('../../fuzzy-torch') # or just install the module\n",
    "sys.path.append('../../fuzzy-tools') # or just install the module\n",
    "sys.path.append('../../astro-lightcurves-handler') # or just install the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "found filedirs: (../../surveys-save/)\n",
      "(0) - ../../surveys-save//survey=alerceZTFv7.1~bands=gr~mode=onlySNe.splcds - 17.073[mbs]\n",
      "(1) - ../../surveys-save//survey=alerceZTFv7.1~bands=gr~mode=onlySNe~method=spm-mcmc-estw.splcds - 226.818[mbs]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "filedirs after searching with filters: (../../surveys-save/)\n",
      "(0) - ../../surveys-save//survey=alerceZTFv7.1~bands=gr~mode=onlySNe.splcds - 17.073[mbs]\n",
      "(1) - ../../surveys-save//survey=alerceZTFv7.1~bands=gr~mode=onlySNe~method=spm-mcmc-estw.splcds - 226.818[mbs]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "from fuzzytools.files import search_for_filedirs\n",
    "from lchandler import _C as _C\n",
    "\n",
    "surveys_rootdir = '../../surveys-save/'\n",
    "filedirs = search_for_filedirs(surveys_rootdir, fext=_C.EXT_SPLIT_LIGHTCURVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "LCDataset:\n",
      "[3@train; samples 1,195]\n",
      "(.) obs_samples=32,482; min_len=6; max_dur=538.8[days]; dur(p50)=52.9[days]; cadence(p50)=1.0[days]\n",
      "(g) obs_samples=14,398; min_len=0; tmax=10.994140625; max_dur=538.8 [days]; dur(p50)=39.0 [days]; cadence(p50)=3.0 [days]\n",
      "(r) obs_samples=18,084; min_len=0; tmax=13.041015625; max_dur=538.7 [days]; dur(p50)=50.0 [days]; cadence(p50)=3.0 [days]\n",
      "   |        | SLSN - 18/1,195 (1.51%)\n",
      "   |█▍      | SNIIbn - 218/1,195 (18.24%)\n",
      "   |██████  | SNIa - 899/1,195 (75.23%)\n",
      "   |▍       | SNIbc - 60/1,195 (5.02%)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[3@val; samples 398]\n",
      "(.) obs_samples=11,234; min_len=6; max_dur=443.9[days]; dur(p50)=53.0[days]; cadence(p50)=1.1[days]\n",
      "(g) obs_samples=4,899; min_len=0; tmax=11.01953125; max_dur=443.9 [days]; dur(p50)=37.0 [days]; cadence(p50)=3.0 [days]\n",
      "(r) obs_samples=6,335; min_len=0; tmax=13.01953125; max_dur=443.9 [days]; dur(p50)=52.0 [days]; cadence(p50)=3.0 [days]\n",
      "   |        | SLSN - 6/398 (1.51%)\n",
      "   |█▍      | SNIIbn - 72/398 (18.09%)\n",
      "   |██████  | SNIa - 300/398 (75.38%)\n",
      "   |▍       | SNIbc - 20/398 (5.03%)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[3@test; samples 399]\n",
      "(.) obs_samples=10,745; min_len=6; max_dur=398.9[days]; dur(p50)=52.9[days]; cadence(p50)=1.0[days]\n",
      "(g) obs_samples=4,748; min_len=0; tmax=10.95703125; max_dur=396.9 [days]; dur(p50)=39.8 [days]; cadence(p50)=3.0 [days]\n",
      "(r) obs_samples=5,997; min_len=0; tmax=13.12890625; max_dur=398.9 [days]; dur(p50)=50.9 [days]; cadence(p50)=3.0 [days]\n",
      "   |        | SLSN - 6/399 (1.50%)\n",
      "   |█▍      | SNIIbn - 73/399 (18.30%)\n",
      "   |██████  | SNIa - 300/399 (75.19%)\n",
      "   |▍       | SNIbc - 20/399 (5.01%)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[3@train.spm-mcmc-estw; samples 38,240]\n",
      "(.) obs_samples=1,014,386; min_len=6; max_dur=539.0[days]; dur(p50)=47.7[days]; cadence(p50)=1.8[days]\n",
      "(g) obs_samples=448,846; min_len=0; tmax=9.869214534759521; max_dur=538.0 [days]; dur(p50)=33.6 [days]; cadence(p50)=2.2 [days]\n",
      "(r) obs_samples=565,540; min_len=0; tmax=11.721720218658447; max_dur=538.2 [days]; dur(p50)=44.2 [days]; cadence(p50)=2.2 [days]\n",
      "   |        | SLSN - 576/38,240 (1.51%)\n",
      "   |█▍      | SNIIbn - 6,976/38,240 (18.24%)\n",
      "   |██████  | SNIa - 28,768/38,240 (75.23%)\n",
      "   |▍       | SNIbc - 1,920/38,240 (5.02%)\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from fuzzytools.files import load_pickle, save_pickle\n",
    "from fuzzytools.files import get_dict_from_filedir\n",
    "\n",
    "method = 'spm-mcmc-estw'\n",
    "filedir = f'../../surveys-save/survey=alerceZTFv7.1~bands=gr~mode=onlySNe~method={method}.splcds'\n",
    "kf = '3'\n",
    "\n",
    "filedict = get_dict_from_filedir(filedir)\n",
    "root_folder = filedict['_rootdir']\n",
    "cfilename = filedict['_cfilename']\n",
    "survey = filedict['survey']\n",
    "lcdataset = load_pickle(filedir)\n",
    "lcdataset.only_keep_kf(kf) # saves ram\n",
    "print(lcdataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcset = lcdataset[f'{kf}@train']\n",
    "values = lcset.get_all_values('obs')\n",
    "print(np.min(values), np.percentile(values, 50), np.max(values))\n",
    "print(np.sort(values)[::-1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcset = lcdataset[f'{kf}@train.{method}']\n",
    "values = lcset.get_all_values('obs')\n",
    "print(np.min(values), np.percentile(values, 50), np.max(values))\n",
    "print(np.sort(values)[::-1][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from lchandler.plots.distrs import plot_values_distribution\n",
    "\n",
    "kf = 0\n",
    "set_name = f'{kf}@train'\n",
    "lcdataset[set_name].set_diff_parallel('days')\n",
    "plot_values_distribution(lcdataset, set_name, 'd_days')\n",
    "plot_values_distribution(lcdataset, set_name, 'obs')\n",
    "plot_values_distribution(lcdataset, set_name, 'obse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lcclassifier.datasets import CustomDataset\n",
    "\n",
    "dataset_kwargs = {\n",
    "    'max_day':100.,\n",
    "    #attrs':['days','obs', 'obse']\n",
    "    'in_attrs':['obs', 'obse'],\n",
    "    #'attrs':['d_days','obs', 'obse']\n",
    "    'rec_attr':'obs',\n",
    "}\n",
    "repeats = 5\n",
    "device = 'cpu'\n",
    "#lcset_name = f'{kf}@train.{method}'\n",
    "lcset_name = f'{kf}@train'\n",
    "s_train_dataset_da = CustomDataset(lcset_name, copy(lcdataset[lcset_name]), device,\n",
    "    balanced_repeats=repeats,\n",
    "    precomputed_copies=2, # 1 8 16\n",
    "    uses_daugm=True,\n",
    "    uses_dynamic_balance=True,\n",
    "    ds_mode={'random':.75, 'left':.0, 'none':.25,},\n",
    "    **dataset_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "s_train_dataset_da.calcule_precomputed(verbose=1, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lcclassifier.datasets import CustomDataset\n",
    "\n",
    "\n",
    "lcset_name = f'{main_args.kf}@train.{main_args.method}'\n",
    "s_train_dataset_da = CustomDataset(lcset_name, copy(lcdataset[lcset_name]), device,\n",
    "    balanced_repeats=repeats,\n",
    "    precomputed_copies=8, # 1 8 16\n",
    "    uses_daugm=True,\n",
    "    uses_dynamic_balance=True,\n",
    "    ds_mode={'random':.75, 'left':.0, 'none':.25,},\n",
    "    **dataset_kwargs,\n",
    "    )\n",
    "\n",
    "device = 'cpu' # cpu\n",
    "train_dataset = CustomDataset(f'{kf}@train.{method}', copy(lcdataset[f'{kf}@train.{method}']), device, **dataset_kwargs)\n",
    "val_dataset = CustomDataset(f'{kf}@val', copy(lcdataset[f'{kf}@val']), device, **dataset_kwargs)\n",
    "train_dataset.transfer_scalers_to(val_dataset) # transfer metadata to val/test\n",
    "print('train_dataset:', train_dataset)\n",
    "print('val_dataset:', val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from fuzzytorch.utils import print_tdict\n",
    "import cProfile\n",
    "\n",
    "p = cProfile.Profile(); p.enable()\n",
    "tdict = train_dataset.get_item(train_dataset.get_lcobj_names()[0])\n",
    "print_tdict(tdict)\n",
    "p.disable(); p.dump_stats('prof.prof')\n",
    "print(tdict['target']['balanced_w'])\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from lchandler.plots.lc import plot_lightcurve\n",
    "from lchandler import C_ as C_\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = train_dataset\n",
    "lcobj_name = dataset.get_random_stratified_lcobj_names()[0]\n",
    "\n",
    "\n",
    "tdict, lcobj = dataset.get_item(lcobj_name, uses_len_clip=False, uses_daugm=False, return_lcobjs=True)\n",
    "print(lcobj)\n",
    "\n",
    "\n",
    "minput = tdict['input']\n",
    "target = tdict['target']\n",
    "\n",
    "figsize = (10,13)\n",
    "fig, axs = plt.subplots(5+1, 1, figsize=figsize)\n",
    "\n",
    "ax = axs[0]\n",
    "for kb,b in enumerate(dataset.band_names):\n",
    "    plot_lightcurve(ax, lcobj, b, label=f'{b} obs', max_day=dataset.max_day)\n",
    "ax.set_ylabel('observation')\n",
    "\n",
    "b = 'r'\n",
    "len_lcobj = minput[f'onehot.{b}'].sum()\n",
    "ax = axs[1]\n",
    "time = minput[f'rtime.{b}'][...,0]\n",
    "for ka,in_attr in enumerate(dataset.in_attrs):\n",
    "    ax.plot(time[:len_lcobj], minput[f'x.{b}'][:len_lcobj,ka], '-o', label=f'{C_.SHORT_NAME_DICT[in_attr]} (norm)')\n",
    "ax.set_ylabel(f'x.{b}')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.plot(time[:len_lcobj], minput[f'onehot.{b}'][:len_lcobj], 'o')\n",
    "ax.set_ylabel(f'onehot.{b}')\n",
    "\n",
    "ax = axs[3]\n",
    "ax.plot(time[:len_lcobj], minput[f'rtime.{b}'][:len_lcobj], '-o')\n",
    "ax.set_ylabel(f'time.{b}')\n",
    "\n",
    "ax = axs[4]\n",
    "ax.plot(time[:len_lcobj], minput[f'dtime.{b}'][:len_lcobj], '-o')\n",
    "ax.set_ylabel(f'dtime.{b}')\n",
    "\n",
    "ax = axs[5]\n",
    "ax.plot(time[:len_lcobj], target[f'rec_x.{b}'][:len_lcobj], '-o')\n",
    "ax.set_ylabel(f'rec_x.{b}')\n",
    "\n",
    "class_name = dataset.class_names[target['y']]\n",
    "title = ''\n",
    "title += f'training light curve sample & model inputs & onehot & temporal encoding \\n'\n",
    "title += f'survey: {dataset.lcset.survey} - set: {dataset.lcset_name}'\n",
    "title += f' - class: {class_name} - max_day: {dataset.max_day:.2f} - max_len: {dataset.max_len}'\n",
    "#title += f' - training: {dataset.training}'\n",
    "for ax in axs:\n",
    "    #ax.legend(prop={'size':14})\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(alpha=0.5)\n",
    "axs[0].set_title(title)\n",
    "axs[-1].set_xlabel('days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lcclassifier.dataloaders import CustomDataLoader\n",
    "from fuzzytorch.utils import print_tdict\n",
    "\n",
    "loader_kwargs = {\n",
    "    'batch_size':2,\n",
    "}\n",
    "random_subcrops = 3\n",
    "s_train_loader = CustomDataLoader(train_dataset, shuffle=False, **loader_kwargs)\n",
    "s_train_loader.eval()\n",
    "dataset.set_max_day(40)\n",
    "\n",
    "for k,tdict in enumerate(s_train_loader):\n",
    "    target = tdict['target']\n",
    "    print_tdict(tdict)\n",
    "    print(tdict['input']['rtime.*'][0,:,0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lcclassifier.dataloaders import CustomDataLoader\n",
    "from fuzzytorch.utils import print_tdict\n",
    "\n",
    "loader_kwargs = {\n",
    "    'batch_size':1,\n",
    "    #'num_workers':1, # bug?\n",
    "}\n",
    "random_subcrops = 3\n",
    "s_train_loader = CustomDataLoader(train_dataset, shuffle=True, random_subcrops=random_subcrops, **loader_kwargs)\n",
    "s_train_loader.train()\n",
    "\n",
    "for k,tdict in enumerate(s_train_loader):\n",
    "    model_input = tdict['input']\n",
    "    target = tdict['target']\n",
    "    print_tdict(tdict)\n",
    "    for idx in range(len(model_input['x'])):\n",
    "        print(model_input['x'][idx,:,0])\n",
    "        print(model_input['onehot'][idx].sum(-1))\n",
    "    assert 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
